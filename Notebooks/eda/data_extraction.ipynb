{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean columns and transform columns to datetime\n",
    "\n",
    "def process_dataframe(df):\n",
    "    # Verifica si la columna 'datetime' existe en el DataFrame\n",
    "    if 'datetime' in df.columns:\n",
    "        # Convierte la columna 'datetime' a tipo datetime con zona horaria UTC\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        \n",
    "        # Elimina la columna 'percentage' si existe\n",
    "        if 'percentage' in df.columns:\n",
    "            df = df.drop('percentage', axis=1)\n",
    "        \n",
    "        # Elimina la información de la zona horaria para que sea similar a tu ejemplo\n",
    "        df['datetime'] = df['datetime'].dt.tz_localize(None)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the Api's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "params = {\n",
    "    'time_trunc': 'hour',\n",
    "    'geo_limit': 'peninsular',\n",
    "    'geo_ids': '8741'\n",
    "}\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://apidatos.ree.es/es/datos/mercados/precios-mercados-tiempo-real?'\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Calculate the start date as one year before the current date\n",
    "start_date = current_datetime - timedelta(days=365)\n",
    "\n",
    "# Initialize a list to store the DataFrames of prices\n",
    "dfs_prices = []\n",
    "\n",
    "# Create a loop to fetch data month by month\n",
    "while start_date <= current_datetime:\n",
    "    # Calculate the end date for the current month\n",
    "    year = start_date.year\n",
    "    month = start_date.month\n",
    "    last_day_of_month = (start_date.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(days=1)\n",
    "    end_date = last_day_of_month.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    # Add the dates to the parameters\n",
    "    params['start_date'] = start_date.isoformat()\n",
    "    params['end_date'] = end_date.isoformat()\n",
    "\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the price data for the current month\n",
    "    monthly_prices = data['included'][0]['attributes']['values']\n",
    "\n",
    "    # Create a DataFrame for the current month and add it to the list\n",
    "    df_month = pd.DataFrame(monthly_prices)\n",
    "    dfs_prices.append(df_month)\n",
    "\n",
    "    # Move to the next month\n",
    "    start_date = last_day_of_month + timedelta(days=1)\n",
    "\n",
    "# Concatenate all the DataFrames into one\n",
    "df_all_prices = pd.concat(dfs_prices, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_prices = process_dataframe(df_all_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_prices.to_csv('../../data/df_yearly_prices.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Monthly demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "params = {\n",
    "    'time_trunc': 'month',\n",
    "    'geo_limit': 'peninsular',\n",
    "    'geo_ids': '8741'\n",
    "}\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Round the current date to the previous night at 00:00\n",
    "end_date = current_datetime.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "\n",
    "# Calculate the start date as one year ago at 00:00\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "# Convert the dates to text strings in the \"00:00\" format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "# Define the base URL with dynamic time parameters\n",
    "base_url = f'https://apidatos.ree.es/es/datos/demanda/demanda-maxima-diaria?start_date={start_date_str}&end_date={end_date_str}'\n",
    "\n",
    "# Make the request to the API\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Extract and print the data\n",
    "df_max_demand = data['included'][0]['attributes']['values']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_max_demand = pd.DataFrame(df_max_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the DataFrame\n",
    "df_max_demand = process_dataframe(df_max_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_max_demand.to_csv('../../data/df_max_demand.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "params = {\n",
    "    'time_trunc': 'hour',\n",
    "    'geo_limit': 'peninsular',\n",
    "    'geo_ids': '8741'\n",
    "}\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://apidatos.ree.es/es/datos/demanda/demanda-tiempo-real?'\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Calculate the start date as one year before the current date\n",
    "start_date = current_datetime - timedelta(days=365)\n",
    "\n",
    "# Initialize a list to store the DataFrames of prices\n",
    "dfs_r_demand = []\n",
    "dfs_p_demand = []\n",
    "dfs_e_demand = []\n",
    "\n",
    "# Create a loop to fetch data month by month\n",
    "while start_date <= current_datetime:\n",
    "    # Calculate the end date for the current month\n",
    "    year = start_date.year\n",
    "    month = start_date.month\n",
    "    last_day_of_month = (start_date.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(days=1)\n",
    "    end_date = last_day_of_month.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    # Add the dates to the parameters\n",
    "    params['start_date'] = start_date.isoformat()\n",
    "    params['end_date'] = end_date.isoformat()\n",
    "\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the price data for the current month\n",
    "    monthly_demand_1 = data['included'][0]['attributes']['values']\n",
    "    monthly_demand_2 = data['included'][1]['attributes']['values']\n",
    "    monthly_demand_3 = data['included'][2]['attributes']['values']\n",
    "\n",
    "    # Create a DataFrame for the current month\n",
    "    df_month_1 = pd.DataFrame(monthly_demand_1)\n",
    "    df_month_2 = pd.DataFrame(monthly_demand_2)\n",
    "    df_month_3 = pd.DataFrame(monthly_demand_3)\n",
    "\n",
    "    # Add the DataFrames to the list\n",
    "    dfs_r_demand.append(df_month_1)\n",
    "    dfs_p_demand.append(df_month_2)\n",
    "    dfs_e_demand.append(df_month_3)\n",
    "\n",
    "    # Move to the next month\n",
    "    start_date = last_day_of_month + timedelta(days=1)\n",
    "\n",
    "# Concatenate all DataFrames in the list to create a single DataFrame\n",
    "df_real_demand = pd.concat(dfs_r_demand, ignore_index=True)\n",
    "df_programed_demand = pd.concat(dfs_p_demand, ignore_index=True)\n",
    "df_expected_demand = pd.concat(dfs_e_demand, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the DataFrames\n",
    "df_real_demand = process_dataframe(df_real_demand)\n",
    "df_expected_demand = process_dataframe(df_expected_demand)\n",
    "df_programed_demand = process_dataframe(df_programed_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV\n",
    "df_real_demand.to_csv('../../data/df_real_demand.csv', index=False)\n",
    "df_expected_demand.to_csv('../../data/df_expected_demand.csv', index=False)\n",
    "df_programed_demand.to_csv('../../data/df_programed_demand.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "params = {\n",
    "    'time_trunc': 'day',\n",
    "    'geo_limit': 'peninsular',\n",
    "    'geo_ids': '8741'\n",
    "}\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://apidatos.ree.es/es/datos/generacion/estructura-generacion?'\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Calculate the start date as one year before the current date\n",
    "params['start_date'] = current_datetime - timedelta(days=365)\n",
    "\n",
    "# Calculate the end date as the current date\n",
    "params['end_date'] = current_datetime\n",
    "\n",
    "# Initialize a dictionary to store DataFrames for each 'type'\n",
    "dfs_structure_by_type = {}\n",
    "\n",
    "# Make the initial HTTP request and store the data\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\2420173044.py:14: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\2420173044.py:14: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\2420173044.py:14: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\2420173044.py:14: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\2420173044.py:14: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\2420173044.py:14: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n"
     ]
    }
   ],
   "source": [
    "# Loop through the 'included' list and store the DataFrames\n",
    "for i in range(len(data['included'])):\n",
    "    type_name = data['included'][i]['type']\n",
    "    df = pd.DataFrame(data['included'][i]['attributes']['values'])\n",
    "    # Renombrar las columnas con un prefijo único basado en el tipo de generación\n",
    "    df = df.rename(columns={'value': f'value_{type_name}'})\n",
    "    dfs_structure_by_type[type_name] = df\n",
    "\n",
    "# Crear un DataFrame consolidado inicialmente con la columna 'datetime'\n",
    "consolidated_df = pd.DataFrame(data['included'][0]['attributes']['values'])[['datetime']]\n",
    "\n",
    "# Fusionar los DataFrames en función de la columna 'datetime'\n",
    "for key, df in dfs_structure_by_type.items():\n",
    "    consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n",
    "\n",
    "# Quitar las columnas que contienen 'percentage'\n",
    "df_structure_merge = consolidated_df.loc[:, ~consolidated_df.columns.str.contains('percentage')]\n",
    "# Quitar todos los strings que sean value_\n",
    "df_structure_merge.columns = df_structure_merge.columns.str.replace('value_', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Hidráulica</th>\n",
       "      <th>Turbinación bombeo</th>\n",
       "      <th>Nuclear</th>\n",
       "      <th>Carbón</th>\n",
       "      <th>Fuel + Gas</th>\n",
       "      <th>Ciclo combinado</th>\n",
       "      <th>Eólica</th>\n",
       "      <th>Solar fotovoltaica</th>\n",
       "      <th>Solar térmica</th>\n",
       "      <th>Otras renovables</th>\n",
       "      <th>Cogeneración</th>\n",
       "      <th>Residuos no renovables</th>\n",
       "      <th>Residuos renovables</th>\n",
       "      <th>Generación total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-25T00:00:00.000+02:00</td>\n",
       "      <td>30080.301</td>\n",
       "      <td>10971.145</td>\n",
       "      <td>119122.962</td>\n",
       "      <td>5995.381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159026.494</td>\n",
       "      <td>244455.554</td>\n",
       "      <td>45358.291</td>\n",
       "      <td>796.962</td>\n",
       "      <td>11195.358</td>\n",
       "      <td>32890.611</td>\n",
       "      <td>3980.7345</td>\n",
       "      <td>2048.0415</td>\n",
       "      <td>665921.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-26T00:00:00.000+02:00</td>\n",
       "      <td>38650.143</td>\n",
       "      <td>7785.445</td>\n",
       "      <td>118898.293</td>\n",
       "      <td>5719.703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202505.778</td>\n",
       "      <td>176570.620</td>\n",
       "      <td>43052.518</td>\n",
       "      <td>374.961</td>\n",
       "      <td>11209.556</td>\n",
       "      <td>33696.408</td>\n",
       "      <td>4227.1300</td>\n",
       "      <td>2063.9650</td>\n",
       "      <td>644754.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-27T00:00:00.000+02:00</td>\n",
       "      <td>23648.834</td>\n",
       "      <td>10923.012</td>\n",
       "      <td>118905.370</td>\n",
       "      <td>3517.236</td>\n",
       "      <td>0.001</td>\n",
       "      <td>94982.595</td>\n",
       "      <td>308167.676</td>\n",
       "      <td>58365.066</td>\n",
       "      <td>2902.327</td>\n",
       "      <td>11302.942</td>\n",
       "      <td>33125.862</td>\n",
       "      <td>3827.6405</td>\n",
       "      <td>1884.2845</td>\n",
       "      <td>671552.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-28T00:00:00.000+02:00</td>\n",
       "      <td>26491.342</td>\n",
       "      <td>13053.460</td>\n",
       "      <td>118980.687</td>\n",
       "      <td>4267.437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108810.815</td>\n",
       "      <td>242578.456</td>\n",
       "      <td>56579.410</td>\n",
       "      <td>1842.236</td>\n",
       "      <td>12150.606</td>\n",
       "      <td>34739.583</td>\n",
       "      <td>3867.4525</td>\n",
       "      <td>1877.6325</td>\n",
       "      <td>625239.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-29T00:00:00.000+02:00</td>\n",
       "      <td>27662.928</td>\n",
       "      <td>14771.582</td>\n",
       "      <td>119102.743</td>\n",
       "      <td>4130.189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74687.542</td>\n",
       "      <td>249073.795</td>\n",
       "      <td>62251.113</td>\n",
       "      <td>4877.024</td>\n",
       "      <td>12212.541</td>\n",
       "      <td>32579.921</td>\n",
       "      <td>4136.1000</td>\n",
       "      <td>1879.4490</td>\n",
       "      <td>607364.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2023-10-21T00:00:00.000+02:00</td>\n",
       "      <td>56829.100</td>\n",
       "      <td>17303.300</td>\n",
       "      <td>111193.000</td>\n",
       "      <td>7356.900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63803.600</td>\n",
       "      <td>202876.800</td>\n",
       "      <td>84769.316</td>\n",
       "      <td>5123.584</td>\n",
       "      <td>6702.700</td>\n",
       "      <td>28985.300</td>\n",
       "      <td>3669.2500</td>\n",
       "      <td>1999.7500</td>\n",
       "      <td>590612.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2023-10-22T00:00:00.000+02:00</td>\n",
       "      <td>57046.500</td>\n",
       "      <td>12121.500</td>\n",
       "      <td>110483.200</td>\n",
       "      <td>7746.600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72234.900</td>\n",
       "      <td>176301.400</td>\n",
       "      <td>37950.063</td>\n",
       "      <td>936.537</td>\n",
       "      <td>7967.800</td>\n",
       "      <td>32733.900</td>\n",
       "      <td>3678.3000</td>\n",
       "      <td>1978.8000</td>\n",
       "      <td>521179.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2023-10-23T00:00:00.000+02:00</td>\n",
       "      <td>74980.900</td>\n",
       "      <td>19090.800</td>\n",
       "      <td>96024.600</td>\n",
       "      <td>9928.700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>153019.300</td>\n",
       "      <td>153475.400</td>\n",
       "      <td>49859.870</td>\n",
       "      <td>317.630</td>\n",
       "      <td>8341.000</td>\n",
       "      <td>44593.200</td>\n",
       "      <td>3817.0500</td>\n",
       "      <td>2013.6500</td>\n",
       "      <td>615462.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2023-10-24T00:00:00.000+02:00</td>\n",
       "      <td>59575.700</td>\n",
       "      <td>4916.700</td>\n",
       "      <td>95985.600</td>\n",
       "      <td>7637.200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77535.500</td>\n",
       "      <td>299245.700</td>\n",
       "      <td>74564.379</td>\n",
       "      <td>3318.621</td>\n",
       "      <td>8202.300</td>\n",
       "      <td>43972.200</td>\n",
       "      <td>3399.4500</td>\n",
       "      <td>2075.4500</td>\n",
       "      <td>680428.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>2023-10-25T00:00:00.000+02:00</td>\n",
       "      <td>46648.200</td>\n",
       "      <td>3290.800</td>\n",
       "      <td>80108.000</td>\n",
       "      <td>6674.600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29633.300</td>\n",
       "      <td>346613.200</td>\n",
       "      <td>63172.667</td>\n",
       "      <td>2329.533</td>\n",
       "      <td>6641.000</td>\n",
       "      <td>26468.600</td>\n",
       "      <td>2625.0000</td>\n",
       "      <td>1683.5000</td>\n",
       "      <td>615888.400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          datetime  Hidráulica  Turbinación bombeo  \\\n",
       "0    2022-10-25T00:00:00.000+02:00   30080.301           10971.145   \n",
       "1    2022-10-26T00:00:00.000+02:00   38650.143            7785.445   \n",
       "2    2022-10-27T00:00:00.000+02:00   23648.834           10923.012   \n",
       "3    2022-10-28T00:00:00.000+02:00   26491.342           13053.460   \n",
       "4    2022-10-29T00:00:00.000+02:00   27662.928           14771.582   \n",
       "..                             ...         ...                 ...   \n",
       "361  2023-10-21T00:00:00.000+02:00   56829.100           17303.300   \n",
       "362  2023-10-22T00:00:00.000+02:00   57046.500           12121.500   \n",
       "363  2023-10-23T00:00:00.000+02:00   74980.900           19090.800   \n",
       "364  2023-10-24T00:00:00.000+02:00   59575.700            4916.700   \n",
       "365  2023-10-25T00:00:00.000+02:00   46648.200            3290.800   \n",
       "\n",
       "        Nuclear    Carbón  Fuel + Gas  Ciclo combinado      Eólica  \\\n",
       "0    119122.962  5995.381         NaN       159026.494  244455.554   \n",
       "1    118898.293  5719.703         NaN       202505.778  176570.620   \n",
       "2    118905.370  3517.236       0.001        94982.595  308167.676   \n",
       "3    118980.687  4267.437         NaN       108810.815  242578.456   \n",
       "4    119102.743  4130.189         NaN        74687.542  249073.795   \n",
       "..          ...       ...         ...              ...         ...   \n",
       "361  111193.000  7356.900         NaN        63803.600  202876.800   \n",
       "362  110483.200  7746.600         NaN        72234.900  176301.400   \n",
       "363   96024.600  9928.700         NaN       153019.300  153475.400   \n",
       "364   95985.600  7637.200         NaN        77535.500  299245.700   \n",
       "365   80108.000  6674.600         NaN        29633.300  346613.200   \n",
       "\n",
       "     Solar fotovoltaica  Solar térmica  Otras renovables  Cogeneración  \\\n",
       "0             45358.291        796.962         11195.358     32890.611   \n",
       "1             43052.518        374.961         11209.556     33696.408   \n",
       "2             58365.066       2902.327         11302.942     33125.862   \n",
       "3             56579.410       1842.236         12150.606     34739.583   \n",
       "4             62251.113       4877.024         12212.541     32579.921   \n",
       "..                  ...            ...               ...           ...   \n",
       "361           84769.316       5123.584          6702.700     28985.300   \n",
       "362           37950.063        936.537          7967.800     32733.900   \n",
       "363           49859.870        317.630          8341.000     44593.200   \n",
       "364           74564.379       3318.621          8202.300     43972.200   \n",
       "365           63172.667       2329.533          6641.000     26468.600   \n",
       "\n",
       "     Residuos no renovables  Residuos renovables  Generación total  \n",
       "0                 3980.7345            2048.0415        665921.835  \n",
       "1                 4227.1300            2063.9650        644754.520  \n",
       "2                 3827.6405            1884.2845        671552.846  \n",
       "3                 3867.4525            1877.6325        625239.117  \n",
       "4                 4136.1000            1879.4490        607364.927  \n",
       "..                      ...                  ...               ...  \n",
       "361               3669.2500            1999.7500        590612.600  \n",
       "362               3678.3000            1978.8000        521179.500  \n",
       "363               3817.0500            2013.6500        615462.100  \n",
       "364               3399.4500            2075.4500        680428.800  \n",
       "365               2625.0000            1683.5000        615888.400  \n",
       "\n",
       "[366 rows x 15 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_structure_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = df['datetime'].dt.tz_localize(None)\n"
     ]
    }
   ],
   "source": [
    "df_structure_merge = process_dataframe(df_structure_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns index 5\n",
    "df_structure_merge = df_structure_merge.drop(df_structure_merge.columns[5], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structure_merge.to_csv('../../data/df_structure_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installed power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "params = {\n",
    "    'time_trunc': 'day',\n",
    "    'geo_limit': 'peninsular',\n",
    "    'geo_ids': '8741'\n",
    "}\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Round the current date to the previous night at 00:00\n",
    "end_date = current_datetime.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "\n",
    "# Calculate the start date as one year ago at 00:00\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "# Convert the dates to text strings in the \"00:00\" format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "# Define the base URL with dynamic time parameters\n",
    "base_url = f'https://apidatos.ree.es/es/datos/generacion/potencia-instalada?start_date={start_date_str}&end_date={end_date_str}'\n",
    "\n",
    "# Make the request to the API\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# create dataframe for each type\n",
    "dfs_power_by_type = {}\n",
    "\n",
    "for i in range(len(data['included'])):\n",
    "    type_name = data['included'][i]['type']\n",
    "\n",
    "    # Initialize a DataFrame for the current type with the name as the key\n",
    "    dfs_power_by_type[type_name] = pd.DataFrame()\n",
    "\n",
    "    # Extract the 'values' data\n",
    "    values = data['included'][i]['attributes']['values']\n",
    "\n",
    "    # Create a DataFrame for the current month\n",
    "    df_month = pd.DataFrame(values)\n",
    "\n",
    "    # Concatenate the current month's data with the type's DataFrame\n",
    "    dfs_power_by_type[type_name] = pd.concat([dfs_power_by_type[type_name], df_month], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\442479188.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the consolidated data\n",
    "consolidated_df1 = pd.DataFrame(columns=['values', 'percentage', 'datetime',])\n",
    "for key, df in dfs_power_by_type.items():\n",
    "    # Renombra las columnas 'values' y 'datetime' para que tengan el nombre del tipo de generación\n",
    "    \n",
    "    # Fusiona el DataFrame actual con el consolidado en función de la columna 'datetime'\n",
    "    consolidated_df1 = pd.merge(consolidated_df1, df, on='datetime', how='outer')\n",
    "\n",
    "\n",
    "df_power_merge = consolidated_df1.loc[:,~consolidated_df1.columns.str.contains('percentage')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = df['datetime'].dt.tz_localize(None)\n"
     ]
    }
   ],
   "source": [
    "df_power_merge = process_dataframe(df_power_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power_merge = df_power_merge.drop(df_power_merge.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV\n",
    "df_power_merge.to_csv('../../data/df_power_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "params = {\n",
    "    'time_trunc': 'day',\n",
    "    'geo_limit': 'peninsular',\n",
    "    'geo_ids': '8741'\n",
    "}\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Round the current date to the previous night at 00:00\n",
    "end_date = current_datetime.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "\n",
    "# Calculate the start date as one year ago at 00:00\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "# Convert the dates to text strings in the \"00:00\" format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "# Define the base URL with dynamic time parameters\n",
    "base_url = f'https://apidatos.ree.es/es/datos/intercambios/todas-fronteras-programados?start_date={start_date_str}&end_date={end_date_str}'\n",
    "\n",
    "# Make the request to the API\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "dfs_exchange_by_country = {}\n",
    "\n",
    "for i in range(len(data['included']) -1):\n",
    "    type_name = data['included'][i]['type']\n",
    "\n",
    "    # Initialize a DataFrame for the current type with the name as the key\n",
    "    dfs_exchange_by_country[type_name] = pd.DataFrame()\n",
    "\n",
    "    # Extract the 'values' data\n",
    "    values = data['included'][i]['attributes']['content'][2]['attributes']['values']\n",
    "\n",
    "    # Create a DataFrame for the current month\n",
    "    df_month = pd.DataFrame(values)\n",
    "\n",
    "    # Concatenate the current month's data with the type's DataFrame\n",
    "    dfs_exchange_by_country[type_name] = pd.concat([dfs_exchange_by_country[type_name], df_month], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame vacío para almacenar los datos consolidados\n",
    "consolidated_df = pd.DataFrame(columns=['value', 'percentage', 'datetime'])\n",
    "\n",
    "# Fusionar los DataFrames en función de la columna 'datetime'\n",
    "for key, df in dfs_exchange_by_country.items():\n",
    "    # Renombrar las columnas 'value' y 'datetime' con el nombre del key del diccionario\n",
    "    df = df.rename(columns={'value': key, 'datetime': 'datetime'})\n",
    "    \n",
    "    # Fusionar el DataFrame actual con el consolidado en función de la columna 'datetime'\n",
    "    if consolidated_df.empty:\n",
    "        consolidated_df = df\n",
    "    else:\n",
    "        consolidated_df = pd.merge(consolidated_df, df, on='datetime', how='outer')\n",
    "\n",
    "# Quitar las columnas que contienen 'percentage'\n",
    "df_exchange_merge = consolidated_df.loc[:, ~consolidated_df.columns.str.contains('percentage')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = df['datetime'].dt.tz_localize(None)\n"
     ]
    }
   ],
   "source": [
    "df_exchange_merge = process_dataframe(df_exchange_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1682716699.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_exchange_merge[columns_to_impute] = imp_mean.fit_transform(df_exchange_merge[columns_to_impute])\n"
     ]
    }
   ],
   "source": [
    "# Columnas de interés para el imputador\n",
    "columns_to_impute = [\"francia\", \"portugal\", \"marruecos\"]\n",
    "\n",
    "# Crear un objeto SimpleImputer para rellenar con la estrategia de la media\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Rellenar las columnas de interés\n",
    "df_exchange_merge[columns_to_impute] = imp_mean.fit_transform(df_exchange_merge[columns_to_impute])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV\n",
    "df_exchange_merge.to_csv('../../data/df_exchange_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy price components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters\n",
    "params = {\n",
    "    'time_trunc': 'month',\n",
    "    'geo_limit': 'peninsular',\n",
    "    'geo_ids': '8741'\n",
    "}\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://apidatos.ree.es/es/datos/mercados/componentes-precio-energia-cierre-desglose?'\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Round the current date to the previous night at 00:00\n",
    "end_date = current_datetime.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "\n",
    "# Calculate the start date as one year ago at 00:00\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "# Convert the dates to text strings in the \"00:00\" format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "params['start_date'] = start_date_str\n",
    "params['end_date'] = end_date_str\n",
    "\n",
    "# Create dictionary to store dataframes\n",
    "dfs_components_daily = {}\n",
    "dfs_components_intradaily = {}\n",
    "\n",
    "\n",
    "# Make the request to the API\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Extract and print the data\n",
    "for i in range(len(data['included'])):\n",
    "    type_name = data['included'][i]['type']\n",
    "\n",
    "    # Initialize a DataFrame for the current type with the name as the key\n",
    "    dfs_components_daily[type_name] = pd.DataFrame()\n",
    "\n",
    "    # Extract the 'values' data\n",
    "    values = data['included'][i]['attributes']['content'][0]['attributes']['values']\n",
    "\n",
    "\n",
    "    # Create a DataFrame for the current month\n",
    "    df_month = pd.DataFrame(values)\n",
    "\n",
    "    # Concatenate the current month's data with the type's DataFrame']\n",
    "    dfs_components_daily[type_name] = pd.concat([dfs_components_daily[type_name], df_month], ignore_index=True)\n",
    "\n",
    "# Extract and print the data\n",
    "for i in range(len(data['included']) - 2):\n",
    "    type_name = data['included'][i]['type']\n",
    "\n",
    "    # Initialize a DataFrame for the current type with the name as the key\n",
    "    dfs_components_intradaily[type_name] = pd.DataFrame()\n",
    "\n",
    "    # Extract the 'values' data\n",
    "    values2 = data['included'][i]['attributes']['content'][1]['attributes']['values']\n",
    "\n",
    "\n",
    "    # Create a DataFrame for the current month\n",
    "    df_month2 = pd.DataFrame(values2)\n",
    "\n",
    "    # Concatenate the current month's data with the type's DataFrame']\n",
    "    dfs_components_intradaily[type_name] = pd.concat([dfs_components_intradaily[type_name], df_month2], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\3281222130.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df3 = pd.merge(consolidated_df3, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\3281222130.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'value_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df3 = pd.merge(consolidated_df3, df, on='datetime', how='outer')\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\3281222130.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df3 = pd.merge(consolidated_df3, df, on='datetime', how='outer')\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the consolidated data\n",
    "consolidated_df3 = pd.DataFrame(columns=['values', 'percentage', 'datetime',])\n",
    "for key, df in dfs_components_daily.items():\n",
    "    # Renombra las columnas 'values' y 'datetime' para que tengan el nombre del tipo de generación\n",
    "    \n",
    "    # Fusiona el DataFrame actual con el consolidado en función de la columna 'datetime'\n",
    "    consolidated_df3 = pd.merge(consolidated_df3, df, on='datetime', how='outer')\n",
    "\n",
    "# drop all the columns that contains 'percentage'\n",
    "df_components_daily_merge = consolidated_df3.loc[:,~consolidated_df3.columns.str.contains('percentage')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = df['datetime'].dt.tz_localize(None)\n"
     ]
    }
   ],
   "source": [
    "df_components_daily_merge = process_dataframe(df_components_daily_merge)\n",
    "df_components_daily_merge = df_components_daily_merge.drop(df_components_daily_merge.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV\n",
    "df_components_daily_merge.to_csv('../../data/df_components_daily_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\2561345479.py:7: FutureWarning: Passing 'suffixes' which cause duplicate columns {'percentage_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  consolidated_df4 = pd.merge(consolidated_df4, df, on='datetime', how='outer')\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the consolidated data\n",
    "consolidated_df4 = pd.DataFrame(columns=['values', 'percentage', 'datetime',])\n",
    "for key, df in dfs_components_intradaily.items():\n",
    "    # Renombra las columnas 'values' y 'datetime' para que tengan el nombre del tipo de generación\n",
    "    \n",
    "    # Fusiona el DataFrame actual con el consolidado en función de la columna 'datetime'\n",
    "    consolidated_df4 = pd.merge(consolidated_df4, df, on='datetime', how='outer')\n",
    "\n",
    "# drop all the columns that contains 'percentage'\n",
    "df_components_intradaily_merge = consolidated_df4.loc[:,~consolidated_df4.columns.str.contains('percentage')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\1134680523.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['datetime'] = df['datetime'].dt.tz_localize(None)\n"
     ]
    }
   ],
   "source": [
    "df_components_intradaily_merge = process_dataframe(df_components_intradaily_merge)\n",
    "df_components_intradaily_merge = df_components_intradaily_merge.drop(df_components_intradaily_merge.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV\n",
    "df_components_intradaily_merge.to_csv('../../data/df_components_intradaily_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data resampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market exchange resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>francia</th>\n",
       "      <th>datetime</th>\n",
       "      <th>portugal</th>\n",
       "      <th>marruecos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75996.034</td>\n",
       "      <td>2022-10-23 22:00:00</td>\n",
       "      <td>-37093.00</td>\n",
       "      <td>-8820.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8705.200</td>\n",
       "      <td>2022-10-24 22:00:00</td>\n",
       "      <td>-17814.90</td>\n",
       "      <td>-12860.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24084.000</td>\n",
       "      <td>2022-10-25 22:00:00</td>\n",
       "      <td>-14708.70</td>\n",
       "      <td>-12660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910.800</td>\n",
       "      <td>2022-10-26 22:00:00</td>\n",
       "      <td>-9926.10</td>\n",
       "      <td>-11388.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27700.800</td>\n",
       "      <td>2022-10-27 22:00:00</td>\n",
       "      <td>-10911.40</td>\n",
       "      <td>-8820.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>44405.500</td>\n",
       "      <td>2023-10-20 22:00:00</td>\n",
       "      <td>-42234.60</td>\n",
       "      <td>-14091.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>32009.200</td>\n",
       "      <td>2023-10-21 22:00:00</td>\n",
       "      <td>-17327.30</td>\n",
       "      <td>-1580.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>21040.950</td>\n",
       "      <td>2023-10-22 22:00:00</td>\n",
       "      <td>-17772.20</td>\n",
       "      <td>-5218.548129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>-32987.600</td>\n",
       "      <td>2023-10-23 22:00:00</td>\n",
       "      <td>13825.85</td>\n",
       "      <td>2333.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>-37886.000</td>\n",
       "      <td>2023-10-24 22:00:00</td>\n",
       "      <td>17199.00</td>\n",
       "      <td>2900.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       francia            datetime  portugal     marruecos\n",
       "0    75996.034 2022-10-23 22:00:00 -37093.00  -8820.000000\n",
       "1     8705.200 2022-10-24 22:00:00 -17814.90 -12860.000000\n",
       "2    24084.000 2022-10-25 22:00:00 -14708.70 -12660.000000\n",
       "3      910.800 2022-10-26 22:00:00  -9926.10 -11388.700000\n",
       "4    27700.800 2022-10-27 22:00:00 -10911.40  -8820.000000\n",
       "..         ...                 ...       ...           ...\n",
       "362  44405.500 2023-10-20 22:00:00 -42234.60 -14091.600000\n",
       "363  32009.200 2023-10-21 22:00:00 -17327.30  -1580.000000\n",
       "364  21040.950 2023-10-22 22:00:00 -17772.20  -5218.548129\n",
       "365 -32987.600 2023-10-23 22:00:00  13825.85   2333.200000\n",
       "366 -37886.000 2023-10-24 22:00:00  17199.00   2900.000000\n",
       "\n",
       "[367 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exchange_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un rango de fechas a nivel horario\n",
    "rango_horario1 = pd.date_range(start=df_exchange_merge['datetime'].min(), end=df_exchange_merge['datetime'].max(), freq='H')\n",
    "\n",
    "# Expandir los datos a nivel horario\n",
    "df_exchange_hourly = pd.DataFrame({'datetime': rango_horario1})\n",
    "df_exchange_hourly = df_exchange_hourly.merge(df_exchange_merge, on='datetime', how='left')\n",
    "\n",
    "# Rellenar los valores diarios a nivel horario, columna por columna\n",
    "columns_to_fill = ['francia', 'marruecos', 'portugal']\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    df_exchange_hourly[column].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Asegurarse de que todos los valores estén llenos\n",
    "df_exchange_hourly.fillna(method='bfill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV\n",
    "df_exchange_hourly.to_csv('../../data/df_exchange_hourly.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure Merge resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 366 entries, 0 to 365\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   datetime                366 non-null    datetime64[ns]\n",
      " 1   Hidráulica              366 non-null    float64       \n",
      " 2   Turbinación bombeo      366 non-null    float64       \n",
      " 3   Nuclear                 366 non-null    float64       \n",
      " 4   Carbón                  366 non-null    float64       \n",
      " 5   Ciclo combinado         366 non-null    float64       \n",
      " 6   Eólica                  366 non-null    float64       \n",
      " 7   Solar fotovoltaica      366 non-null    float64       \n",
      " 8   Solar térmica           366 non-null    float64       \n",
      " 9   Otras renovables        366 non-null    float64       \n",
      " 10  Cogeneración            366 non-null    float64       \n",
      " 11  Residuos no renovables  366 non-null    float64       \n",
      " 12  Residuos renovables     366 non-null    float64       \n",
      " 13  Generación total        366 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(13)\n",
      "memory usage: 42.9 KB\n"
     ]
    }
   ],
   "source": [
    "df_structure_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un rango de fechas a nivel horario\n",
    "rango_horario = pd.date_range(start=df_structure_merge['datetime'].min(), end=df_structure_merge['datetime'].max(), freq='H')\n",
    "\n",
    "# Expandir los datos a nivel horario\n",
    "df_structure_hourly = pd.DataFrame({'datetime': rango_horario})\n",
    "df_structure_hourly = df_structure_hourly.merge(df_structure_merge, on='datetime', how='left')\n",
    "\n",
    "# Rellenar los valores diarios a nivel horario, columna por columna\n",
    "columns_to_fill = ['Hidráulica', 'Turbinación bombeo','Nuclear', 'Carbón', 'Ciclo combinado', 'Eólica', 'Solar fotovoltaica',\n",
    "                    'Solar térmica', 'Otras renovables', 'Cogeneración', 'Residuos no renovables',\n",
    "                     'Residuos renovables', 'Generación total']\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    df_structure_hourly[column].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Asegurarse de que todos los valores estén llenos\n",
    "df_structure_hourly.fillna(method='bfill', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV\n",
    "df_structure_hourly.to_csv('../../data/df_structure_hourly.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demand resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_demand = df_real_demand.merge(df_programed_demand, on='datetime', how='inner') \\\n",
    "    .merge(df_expected_demand, on='datetime', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_demand.rename(columns={'value_x': 'real_de'}, inplace=True)\n",
    "df_merge_demand.rename(columns={'value_y': 'prog_de'}, inplace=True)\n",
    "df_merge_demand.rename(columns={'value': 'exp_de'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move value_x to the end\n",
    "df_merge_demand = df_merge_demand[['datetime', 'prog_de', 'exp_de', 'real_de']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yearly_prices = pd.read_csv('../../data/df_yearly_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yearly_prices['datetime'] = pd.to_datetime(df_yearly_prices['datetime'])\n",
    "df_yearly_prices['datetime'] = df_yearly_prices['datetime'].dt.tz_localize(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge= df_merge_demand.merge(df_yearly_prices, on='datetime', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_merge.merge(df_structure_hourly, on='datetime', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_merge.merge(df_exchange_hourly, on='datetime', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_38680\\3629397609.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_merge.corr()['value'].sort_values(ascending=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "value                     1.000000\n",
       "Ciclo combinado           0.548996\n",
       "Carbón                    0.481342\n",
       "Otras renovables          0.479792\n",
       "prog_de                   0.470350\n",
       "exp_de                    0.464427\n",
       "real_de                   0.463098\n",
       "Residuos no renovables    0.320079\n",
       "Cogeneración              0.300333\n",
       "portugal                  0.188581\n",
       "Hidráulica                0.172758\n",
       "Residuos renovables       0.153651\n",
       "Generación total          0.118149\n",
       "marruecos                 0.075704\n",
       "Nuclear                   0.068232\n",
       "francia                  -0.030176\n",
       "Solar térmica            -0.175854\n",
       "Solar fotovoltaica       -0.179699\n",
       "Eólica                   -0.284648\n",
       "Turbinación bombeo       -0.350245\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.corr()['value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move column value to column index 1\n",
    "\n",
    "df_merge = df_merge[['datetime', 'value','prog_de', 'exp_de','real_de',\n",
    "                     \t'Hidráulica', 'Turbinación bombeo', 'Nuclear', 'Carbón',\n",
    "                        'Ciclo combinado', 'Eólica', 'Solar fotovoltaica',\t'Solar térmica', 'Otras renovables',\n",
    "                        'Cogeneración',\t'Residuos no renovables', 'Residuos renovables',\n",
    "                        'Generación total',\t'francia', 'portugal', 'marruecos']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_csv('../../data/df_merge_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
